# 相关思考

## 同策略和异策略

本质区别在于，你要训练学习的目标策略，是不是和当前用于收集经验和数据的行为策略一致；一致则为同策略，不一致则为异策略；

# 项目抽象

## 整体思路

将项目整体拆分为以下几个模块：

* agents：
  * 主要包含基于各类算法实现的agent类，比如DqnAgent，PPOAgent等，agent核心可以通过与环境的交互，来不断更新自己的策略，所以该类需要包含以下核心方法：
    * 选择动作：从动作空间，按照指定策略选择动作，然后交给环境执行；选择动作可以是随机的，也可以是在策略网络的指导下
    * 更新策略：更新指导agent的策略，更新的方式可以是软更新，或者直接更新；
* networks：
  * 策略网络，可以是神经网络，也可以是Table，指导agent选择动作，是agent的核心组件之一，所以该类需要包含以下核心方法：
    * 选择动作：在策略网络指导下，选择动作；
    * 更新策略：具体执行策略参数的更新；
* scripts：
  * 里面包含统一的训练脚本、验证脚本等；
  * trains：包含对agent各类训练脚本，训练的核心步骤如下：
    * 首先定义训练所需智能体、各类超参数，episodes等
    * for循环体：
      * 初始化环境
      * agent选择动作
      * 环境执行动作，并得到反馈(reward、done、truncated、info)
      * agent收集反馈-
        * 要是将优先级经验回放做成统一的技巧，传统的无非是经验池大小是1的无优先级的经验回放
      * agent根据反馈进行网络/策略更新
      * 更新当前状态
    * 其他监控代码
      * 通用打印模块+日志模块
      * checkpoints保存模块
      * 通用tensorboard可视化记录模块
        * 每一个episode：得到的总奖励、总长度、耗时、探索率
  * evals：包含对agent各类验证脚本，验证的核心步骤，整体与trains类似，只是agent不会进行策略更新
* utils：里面包含统一的工具类和工具函数；
* logs：里面包含训练、验证等各类日志和记录；
* checkpoints：里面包含训练断点，需要记录完整训练信息，便于断点续练；

## agents分类

### 基础agent

### 基于价值的

* QTable_QLearning
* QTable_SARSA
* DQN
* Double_DQN
* Rainbow_DQN

### 基于策略的

* actor-critic
* A2C
* PPO
* SAC
*
